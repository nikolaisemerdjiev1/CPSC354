\documentclass{article}
\documentclass[11pt]{article}

\usepackage{tikz} 
\usetikzlibrary{automata, positioning, arrows} 

\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fullpage}
\usepackage{color}
\usepackage{parskip}
\usepackage{hyperref}
  \hypersetup{
    colorlinks = true,
    urlcolor = blue,       % color of external links using \href
    linkcolor= blue,       % color of internal links 
    citecolor= blue,       % color of links to bibliography
    filecolor= blue,        % color of file links
    }
\usepackage{lmodern}
\usepackage{qtree}     % <-- classic tree package
\usepackage{graphicx}  % <-- for \resizebox 
\usepackage{listings}
\usepackage[utf8]{inputenc}                                                    
\usepackage[T1]{fontenc}  
\usepackage{quiver}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\newcommand{\lam}{\lambda}
\newcommand{\map}{\Rightarrow}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=haskell,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newtheoremstyle{theorem}
  {\topsep}   % ABOVESPACE
  {\topsep}   % BELOWSPACE
  {\itshape\/}  % BODYFONT
  {0pt}       % INDENT (empty value is the same as 0pt)
  {\bfseries} % HEADFONT
  {.}         % HEADPUNCT
  {5pt plus 1pt minus 1pt} % HEADSPACE
  {}          % CUSTOM-HEAD-SPEC
\theoremstyle{theorem} 
   \newtheorem{theorem}{Theorem}[section]
   \newtheorem{corollary}[theorem]{Corollary}
   \newtheorem{lemma}[theorem]{Lemma}
   \newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
   \newtheorem{definition}[theorem]{Definition}
   \newtheorem{example}[theorem]{Example}
\theoremstyle{remark}    
  \newtheorem{remark}[theorem]{Remark}




\title{CPSC-354 Report}
\author{Nikolai Semerdjiev  \\ Chapman University}

\date{\today} 

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\setcounter{tocdepth}{3}
\tableofcontents

\section{Introduction}\label{intro}

\section{Week by Week}\label{homework}

\subsection{Week 1}

\subsubsection{Notes and Exploration}

In Week 1, we began with the MIU (or MU) puzzle from Hofstadter’s *Gödel, Escher, Bach*. 
This puzzle introduces the idea of formal systems and rules of inference. 
It is a good starting point to think about what it means to derive a string from an axiom 
under a fixed set of rules.

\begin{center}
    \textbf{The MU Puzzle}
\end{center}

\textbf{RULES:}
\begin{enumerate}
  \item (Append-\texttt{U}) If a string ends with \texttt{I}, you may append \texttt{U}:\\
  \hspace{1em} $x\texttt{I} \to x\texttt{IU}$.
  \item (Double) From \texttt{M}$x$ you may produce \texttt{M}$xx$:\\
  \hspace{1em} $\texttt{M}x \to \texttt{M}xx$.
  \item (III$\to$U) Replace any occurrence of \texttt{III} with \texttt{U}:\\
  \hspace{1em} $x\texttt{III}y \to x\texttt{U}y$.
  \item (Delete \texttt{UU}) Delete any occurrence of \texttt{UU}:\\
  \hspace{1em} $x\texttt{UU}y \to xy$.
\end{enumerate}

\subsubsection{Homework}

\textbf{Problem:} Can you derive \texttt{MU} from \texttt{MI}?

\textbf{Solution:} No, it is impossible to derive \texttt{MU} from \texttt{MI}. At first, 
playing around with the rules, I noticed that the goal was to create the correct number 
of \texttt{I}'s so that they could be converted to a single \texttt{U}. This means we 
would need $N_I \bmod 3 = 0$, where $N_I$ counts the \texttt{I}'s.

Now, consider how each rule affects $N_I$:
\begin{itemize}
  \item \textbf{(Append-\texttt{U})} Appends a \texttt{U}, leaves $N_I$ unchanged.
  \item \textbf{(Double)} $\texttt{M}x \to \texttt{M}xx$ doubles $N_I$; in modular arithmetic, $N_I \mapsto 2N_I \pmod{3}$.
  \item \textbf{(III$\to$U)} Removes three \texttt{I}'s, leaving $N_I \bmod 3$ unchanged.
  \item \textbf{(Delete \texttt{UU})} Only touches \texttt{U}'s, leaves $N_I$ unchanged.
\end{itemize}

Starting from $\texttt{MI}$, we have $N_I = 1 \equiv 1 \pmod{3}$. Doubling cycles 
between $1$ and $2$ modulo $3$, never producing $0$. Thus, it is impossible to reach 
$N_I \equiv 0 \pmod{3}$.

Since $\texttt{MU}$ has $N_I=0$, it cannot be derived from $\texttt{MI}$.

\subsubsection{Questions}

What is the reasoning behind being able to convert $\texttt{MIII}$ into $\texttt{MU}$ 
(using the rule $\texttt{III}\to \texttt{U}$) but not being able to go the other way 
(from $\texttt{MU}$ to $\texttt{MIII}$)?

\subsection{Week 2}

\subsubsection{Homework}

\textbf{Problem:} Consider the following ARSs. Draw a picture for each one. 
Are the ARSs terminating? Are they confluent? Do they have unique normal forms?

\begin{enumerate}
  \item $A = \{\}, \quad R = \{\}$
  \item $A = \{a\}, \quad R = \{\}$
  \item $A = \{a\}, \quad R = \{(a,a)\}$
  \item $A = \{a,b,c\}, \quad R = \{(a,b),(a,c)\}$
  \item $A = \{a,b\}, \quad R = \{(a,a),(a,b)\}$
  \item $A = \{a,b,c\}, \quad R = \{(a,b),(b,b),(a,c)\}$
  \item $A = \{a,b,c\}, \quad R = \{(a,b),(b,b),(a,c),(c,c)\}$
\end{enumerate}

\textbf{Solution:}  
For each ARS, I drew a graph (see figures) and analyzed:

- **Termination:** whether there are infinite chains.  
- **Confluence:** whether every divergence can rejoin.  
- **Unique normal forms:** whether each element has a unique NF.

I will include one diagram for each ARS along with a short explanation of my analysis.

\paragraph{ARS 1: $A = \{\}, \; R = \{\}$}

There is no infinite chain, no diverging paths exist as there is no path at all, and 
nothing exists to violate uniqueness. Therefore $\checkmark$ terminating, $\checkmark$ confluent, 
and $\checkmark$ has unique normal form.

\paragraph{ARS 2: $A = \{a\}, \; R = \{\}$}

% https://q.uiver.app/#q=WzAsMSxbMCwwLCJcXGJ1bGxldCJdXQ==
\[\begin{tikzcd}
	\bullet
\end{tikzcd}\]

No rewrite steps: no infinite chain, no diverging paths exist or no path at all so it is vacuously satisfied and $a$ is the only reachable normal form from $a$. Therefore $\checkmark$ terminating, $\checkmark$ confluent, and $\checkmark$ has unique normal form.

\paragraph{ARS 3: $A=\{a\},\; R=\{(a,a)\}$}

% https://q.uiver.app/#q=WzAsMSxbMCwwLCJcXGJ1bGxldCJdLFswLDBdXQ==
\[\begin{tikzcd}
	\bullet
	\arrow[from=1-1, to=1-1, loop, in=55, out=125, distance=10mm]
\end{tikzcd}\]

There is an infinite chain, $a \to^* a$. Since $a \to^* y$ and $a \to^* z$, therefore $y = z = a$, joining at $a$, and there are no normal forms as there is only one term, $a$, which has infinite outgoing rewrite steps. Therefore $\times$ terminating, $\checkmark$ confluent, and $\times$ unique normal form.

\paragraph{ARS 4: $A=\{a,b,c\},\; R=\{(a,b),(a,c)\}$}

% https://q.uiver.app/#q=WzAsMyxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzIsMSwiXFxidWxsZXQiXSxbMCwxXSxbMCwyXV0=
\[
\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
\end{tikzcd}
\]

There is no infinite chain, two diverging paths that do not get joined, and two different normal forms from $a$. Therefore $\checkmark$ terminating, $\times$ confluent, and $\times$ unique normal form.

\paragraph{ARS 5: $A=\{a,b\},\; R=\{(a,a),(a,b)\}$}

% https://q.uiver.app/#q=WzAsMixbMCwwLCJcXGJ1bGxldCJdLFsxLDAsIlxcYnVsbGV0Il0sWzAsMF0sWzAsMV1d
\[
\begin{tikzcd}
	\bullet & \bullet
	\arrow[from=1-1, to=1-1, loop, in=55, out=125, distance=10mm]
	\arrow[from=1-1, to=1-2]
\end{tikzcd}
\]

Even with one infinite chain it does not terminate, $a\to a$ and $a\to b$ and they join at $b$, $b$ is the only normal form since $a$ has an infinite chain therefore $\times$ terminating, $\checkmark$ confluent, has a $\checkmark$ unique normal form.


\paragraph{ARS 6: $A=\{a,b,c\},\; R=\{(a,b),(b,b),(a,c)\}$}

% https://q.uiver.app/#q=WzAsMyxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzIsMSwiXFxidWxsZXQiXSxbMCwxXSxbMSwxXSxbMCwyXV0=
\[
\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
	\arrow[from=2-1, to=2-1, loop, in=55, out=125, distance=10mm]
\end{tikzcd}
\]

Infinite chain at $b$ (since $b \to b \to b \cdots$). From $a$ the system diverges to $b$ and $c$ with no connecting path to join them. The only normal form is $c$, but not every element reduces to a unique normal form, so the system is $\times$ terminating, $\times$ confluent, and $\times$ has unique normal form.

\paragraph{ARS 7: $A=\{a,b,c\},\; R=\{(a,b),(a,c),(b,b),(c,c)\}$}

% https://q.uiver.app/#q=WzAsMyxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzIsMSwiXFxidWxsZXQiXSxbMCwxXSxbMCwyXSxbMSwxXSxbMiwyXV0=
\[
\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
	\arrow[from=2-1, to=2-1, loop, in=55, out=125, distance=10mm]
	\arrow[from=2-3, to=2-3, loop, in=55, out=125, distance=10mm]
\end{tikzcd}
\]

There are two infinite chains ($b \to b \to \cdots$ and $c \to c \to \cdots$). 
From $a$ the system diverges to $b$ and $c$, but since $b$ only reaches $b$ and $c$ only reaches $c$, 
the peak at $a$ does not join. Every element has outgoing rewrite steps, so there are no normal forms. 
Therefore $\times$ terminating, $\times$ confluent, and $\times$ has unique normal form.

\textbf{All Eight Combinations}

The homework also asked to find examples of ARSs for each of the eight possible
combinations of confluence, termination, and unique normal forms. 
The table below summarizes the results.

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Confluent} & \textbf{Terminating} & \textbf{Unique Normal Forms} & \textbf{Example $(A,R)$} \\
\hline
True  & True  & True  & $A = \{a,b,c,d\},\; R = \{(a,b),(a,c),(b,d),(c,d)\}$ \\
\hline
True  & True  & False & N/A \\
\hline
True  & False & True  & $A = \{a,b,c,d\},\; R = \{(a,a),(a,b),(a,c),(b,d),(c,d)\}$ \\
\hline
True  & False & False & $A = \{a,b,c,d\},\; R = \{(a,b),(a,c),(b,d),(c,d),(d,d)\}$ \\
\hline
False & True  & True  & N/A \\
\hline
False & True  & False & $A = \{a,b,c,d\},\; R = \{(a,b),(a,c),(b,d)\}$ \\
\hline
False & False & True  & $A = \{a,b,c\},\; R = \{(a,b),(a,c),(b,d),(c,d),(d,d)\}$ \\
\hline
False & False & False & $A = \{a,b,c\},\; R = \{(a,a),(a,b),(a,c)\}$ \\
\hline
\end{tabular}
\end{center}

\textbf{Example Graphs}

\paragraph{Graph 1}
% https://q.uiver.app/#q=WzAsNCxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzEsMiwiXFxidWxsZXQiXSxbMiwxLCJcXGJ1bGxldCJdLFswLDFdLFswLDNdLFsxLDJdLFszLDJdXQ==
\[\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet \\
	& \bullet
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
	\arrow[from=2-1, to=3-2]
	\arrow[from=2-3, to=3-2]
\end{tikzcd}\]

\paragraph{Graph 2}
This comination is impossible as termination gives a normal form for each element AND confluence guarantees any two reduction paths from the same element to join therefore all reductions end at some normal form.

\paragraph{Graph 3}
% https://q.uiver.app/#q=WzAsNCxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzEsMiwiXFxidWxsZXQiXSxbMiwxLCJcXGJ1bGxldCJdLFswLDBdLFswLDFdLFsxLDJdLFswLDNdLFszLDJdXQ==
\[\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet \\
	& \bullet
	\arrow[from=1-2, to=1-2, loop, in=55, out=125, distance=10mm]
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
	\arrow[from=2-1, to=3-2]
	\arrow[from=2-3, to=3-2]
\end{tikzcd}\]

\paragraph{Graph 4}
% https://q.uiver.app/#q=WzAsNCxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzEsMiwiXFxidWxsZXQiXSxbMiwxLCJcXGJ1bGxldCJdLFswLDNdLFswLDFdLFsxLDJdLFszLDJdLFsyLDJdXQ==
\[\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet \\
	& \bullet
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
	\arrow[from=2-1, to=3-2]
	\arrow[from=2-3, to=3-2]
	\arrow[from=3-2, to=3-2, loop, in=55, out=125, distance=10mm]
\end{tikzcd}\]

\paragraph{Graph 5}

This case is impossible. Every system that is terminating and where every element has a unique normal form must also be confluent. 
Indeed, if $a \to^* y$ and $a \to^* z$, then both $y$ and $z$ reduce to some normal forms. 
Since the normal form is unique, $y$ and $z$ must reduce to the same normal form, 
which means the system is confluent.


\paragraph{Graph 6}
% https://q.uiver.app/#q=WzAsNCxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzAsMiwiXFxidWxsZXQiXSxbMiwxLCJcXGJ1bGxldCJdLFswLDFdLFsxLDJdLFswLDNdXQ==
\[\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet \\
	\bullet
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
	\arrow[from=2-1, to=3-1]
\end{tikzcd}\]

\paragraph{Graph 7}
% https://q.uiver.app/#q=WzAsMyxbMSwwLCJcXGJ1bGxldCJdLFswLDEsIlxcYnVsbGV0Il0sWzIsMSwiXFxidWxsZXQiXSxbMCwwXSxbMCwxXSxbMCwyXSxbMiwyXV0=
\[\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet
	\arrow[from=1-2, to=1-2, loop, in=55, out=125, distance=10mm]
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
	\arrow[from=2-3, to=2-3, loop, in=55, out=125, distance=10mm]
\end{tikzcd}\]

\paragraph{Graph 8}
% https://q.uiver.app/#q=WzAsMyxbMSwwLCJcXGJ1bGxldCJdLFsyLDEsIlxcYnVsbGV0Il0sWzAsMSwiXFxidWxsZXQiXSxbMCwwXSxbMCwyXSxbMCwxXV0=
\[\begin{tikzcd}
	& \bullet \\
	\bullet && \bullet
	\arrow[from=1-2, to=1-2, loop, in=55, out=125, distance=10mm]
	\arrow[from=1-2, to=2-1]
	\arrow[from=1-2, to=2-3]
\end{tikzcd}\]



\subsubsection{Questions}

How can thinking about ARSs help us better understand the way programming languages define and control the process of evaluating programs?

\subsection{Week 3}

\subsubsection{Homework}

\textbf{Exercise 5:} Consider the rewrite rules
\[
ab \to ba, \quad ba \to ab, \quad aa \to \varepsilon, \quad b \to \varepsilon
\]

Reduce some example strings such as \texttt{abba} and \texttt{bababa}.  
\textbf{Answer:} 
\[
\texttt{abba} \;\to\; \texttt{abab} \;\to\; \texttt{aba} \;\to\; \texttt{aa} \;\to\; \varepsilon.
\]
\[
\texttt{bababa} \;\to\; \texttt{bbaaba} \;\to\; \texttt{bbba} \;\to\; \texttt{bba} \;\to\; \texttt{ba} \;\to\; \texttt{a}.
\]
\emph{No rule applies to a single \texttt{a}; this is the farthest you can go.}\\[1em]

Why is the ARS not terminating?  
\textbf{Answer:} There is always a way to terminate if you use the deletion rules (3) and (4), but using only the swap rules (1) and (2) yields an infinite loop (e.g., \(\texttt{ab} \leftrightarrow \texttt{ba} \leftrightarrow \texttt{ab} \leftrightarrow \cdots\)), so the ARS is \emph{not terminating} overall.\\[1em]

Find two strings that are not equivalent. How many non-equivalent strings can you find?  
\textbf{Answer:} Two strings not equivalent: \(\texttt{baba} \not\!\leftrightarrow^{*} \texttt{ababa}\). There are infinitely many non-equivalent pairs: strings split by the parity of the number of \(a\)'s (even vs.\ odd), so you can keep making pairs with different \(a\)-parity.\\[1em]

How many equivalence classes does \(\leftrightarrow^{*}\) have? Can you describe them in a nice way? What are the normal forms?  
\textbf{Answer:} There are \(2\) equivalence classes under \(\leftrightarrow^{*}\):
(1) strings with an \emph{even} number of \(a\)'s, which reduce to the normal form \(\varepsilon\);
(2) strings with an \emph{odd} number of \(a\)'s, which reduce to the normal form \(a\).
Set of normal forms: \(\{\varepsilon, a\}\).\\[1em]

Can you modify the ARS so that it becomes terminating without changing its equivalence classes?  
\textbf{Answer:} Yes. Drop \(ab \to ba\) and keep \(ba \to ab\), together with \(aa \to \varepsilon\) and \(b \to \varepsilon\).
This orientation is terminating while preserving the same equivalence classes and normal forms (still \(\{\varepsilon, a\}\)).\\[1em]

Write down a question or two about strings that can be answered using the ARS. Think about whether this amounts to giving a semantics to the ARS.  
\textbf{Answer:} Given any string \(x \in \{a,b\}^{*}\), does \(x\) reduce to \(\varepsilon\) or to \(a\)?
Equivalently: is the number of \(a\)'s in \(x\) even or odd? This gives a semantics mapping each string to \(\varepsilon\) (even) or \(a\) (odd).\\[1em]

\textbf{Exercise 5b:} Consider the rewrite rules
\[
ab \to ba, \quad ba \to ab, \quad aa \to a, \quad b \to \varepsilon
\]

Reduce some example strings such as \texttt{abba} and \texttt{bababa}.  
\textbf{Answer:}
\[
\texttt{abba} \;\to\; \texttt{aba} \;\to\; \texttt{aa} \;\to\; a.
\]
\[
\texttt{bababa} \;\to\; \texttt{ababa} \;\to\; \texttt{aaba} \;\to\; \texttt{aaa} \;\to\; \texttt{aa} \;\to\; a.
\]
\emph{No rule applies to a single \texttt{a}; this is the farthest you can go.}\\[1em]

Why is the ARS not terminating?  
\textbf{Answer:} This example still has an infinite chain if you don’t use rules (3) or (4): the swaps \(ab \leftrightarrow ba\) can loop forever.\\[1em]

Find two strings that are not equivalent. How many non-equivalent strings can you find?  
\textbf{Answer:} Two non-equivalent strings: \(\texttt{bbb} \not\!\leftrightarrow^{*} \texttt{babab}\). There are infinitely many non-equivalent pairs.\\[1em]

How many equivalence classes does \(\leftrightarrow^{*}\) have? Can you describe them in a nice way? What are the normal forms?  
\textbf{Answer:} There are \(2\) equivalence classes for \(\leftrightarrow^{*}\).  
If a string has \emph{no} \(a\)’s, its normal form is \(\varepsilon\).  
If a string has \emph{at least one} \(a\), its normal form is \(a\).  
Set of normal forms: \(\{\varepsilon, a\}\).\\[1em]

Can you modify the ARS so that it becomes terminating without changing its equivalence classes?  
\textbf{Answer:} Drop \(ab \to ba\) and keep \(ba \to ab\), along with \(aa \to a\) and \(b \to \varepsilon\).  
This orientation terminates while preserving the same equivalence classes and normal forms.\\[1em]

Write down a question or two about strings that can be answered using the ARS. Think about whether this amounts to giving a semantics to the ARS.  
\textbf{Answer:} Given any string \(x \in \{a,b\}^{*}\), does \(x\) contain at least one \(a\)?  
Equivalently: does \(x\) reduce to \(a\) (some \(a\) present) or to \(\varepsilon\) (no \(a\)’s)?\\[1em]

\subsubsection{Questions}

How does changing the rewrite rule from $aa \to \varepsilon$ to $aa \to a$ resemble differences in evaluation strategies when creating a programming language?

\subsection{Week 4}

\subsubsection{Homework}

\begin{verbatim}
while b != 0:
    temp = b
    b = a % b
    a = temp
return a
\end{verbatim}

The algorithm always terminates provided that.
\begin{enumerate}
    \item The inputs satisfy $a \geq 0$ and $b \geq 0$.
    \item If $b = 0$ initially, the loop terminates immediately.
    \item For $b > 0$, the remainder operation is defined by the division algorithm: 
    \[
    a = qb + r, \quad 0 \leq r < b,
    \]
    where $r = a \bmod b$.
\end{enumerate}

Define the measure function
\[
\phi(a,b) = b.
\]

Consider one loop iteration:
\[
(a,b) \mapsto (b,\, a \bmod b).
\]
By the division algorithm, 
\[
0 \leq a \bmod b < b.
\]
Therefore,
\[
\phi(a,b) = b > a \bmod b = \phi(b, a \bmod b).
\]
Hence $\phi$ strictly decreases with each iteration.

Since $\phi(a,b)$ is a nonnegative integer, it cannot decrease infinitely. After finitely many steps, we must reach $b = 0$, at which point the loop terminates.

\begin{verbatim}
function merge_sort(arr, left, right):
    if left >= right:
        return
    mid = floor((left + right) / 2)
    merge_sort(arr, left, mid)
    merge_sort(arr, mid+1, right)
    merge(arr, left, mid, right)
\end{verbatim}

The function
\[
\phi(\text{left},\text{right}) \;=\; \text{right} - \text{left} + 1
\]
is a measure function for \texttt{merge\_sort} (with integer indices and
\(\texttt{mid}=\lfloor ( \text{left}+\text{right})/2 \rfloor\)).

Assume \(\text{left},\text{right}\in\mathbb{Z}\) and the procedure is only called with
\(\text{left}\le \text{right}\). The branching factor is finite (at most two recursive calls).

Let \(n=\phi(\text{left},\text{right})=\text{right}-\text{left}+1\).

\begin{itemize}
  \item \emph{Base case.} If \(\text{left}\ge \text{right}\) then \(n\le 1\) and the function returns without making recursive calls. No decrease condition needs to be checked.

  \item \emph{Recursive case.} Suppose \(\text{left}<\text{right}\), hence \(n\ge 2\).
  Set \(\texttt{mid}=\left\lfloor \dfrac{\text{left}+\text{right}}{2}\right\rfloor\).
  Then \(\text{left}\le \texttt{mid}<\text{right}\), so both subproblems are well-formed:
  \[
  (\text{left},\texttt{mid}) \quad\text{and}\quad (\texttt{mid}+1,\text{right}).
  \]
  Their measures are
  \[
  \phi(\text{left},\texttt{mid})=\texttt{mid}-\text{left}+1
  \quad\text{and}\quad
  \phi(\texttt{mid}+1,\text{right})=\text{right}-\texttt{mid}.
  \]
  Using \(\texttt{mid}=\left\lfloor \dfrac{\text{left}+\text{right}}{2}\right\rfloor\) and \(n=\text{right}-\text{left}+1\), we get the bounds
  \[
  \phi(\text{left},\texttt{mid}) \;\le\; \left\lceil \frac{n}{2} \right\rceil
  \quad\text{and}\quad
  \phi(\texttt{mid}+1,\text{right}) \;\le\; \left\lfloor \frac{n}{2} \right\rfloor.
  \]
  Since \(n\ge 2\), both \(\left\lceil \dfrac{n}{2}\right\rceil<n\) and \(\left\lfloor \dfrac{n}{2}\right\rfloor<n\) hold. Therefore
  \[
  \phi(\text{left},\texttt{mid}) \;<\; \phi(\text{left},\text{right})
  \qquad\text{and}\qquad
  \phi(\texttt{mid}+1,\text{right}) \;<\; \phi(\text{left},\text{right}).
  \]
  Hence the measure strictly decreases along every recursive edge.
\end{itemize}

Because \(\phi\) maps states to \(\mathbb{N}\) and strictly decreases with each recursive call,
no infinite descent is possible. Together with finite branching, this implies termination.

\subsubsection{Questions}

How do different evaluation strategies in programming languages (such as call-by-value vs.\ call-by-name) influence whether a program is guaranteed to terminate, and in what ways can techniques like measure functions help us reason about termination across these strategies?

\subsection{Week 5}

\subsubsection{Homework}

\[
\textbf{Goal: }\;(\lambda f.\,\lambda x.\, f\,(f\,x))\;(\lambda f.\,\lambda x.\, f\,(f\,(f\,x))).
\]

\textbf{α–renaming.}
To avoid variable capture, rename bound variables so the two arguments use distinct names:
\[
A \;\equiv\; \lambda f.\,\lambda x.\, f(f\,x)
\quad\text{and}\quad
B \;\equiv\; \lambda g.\,\lambda y.\, g(g(g\,y)).
\]
The term is \(A\,B\).

\medskip
\textbf{β–reduction (outer application).}
\[
A\,B
\;\to_\beta\;
\lambda x.\, B\bigl(B\,x\bigr)
\quad
\text{(substitute } f := B \text{ in } f(f\,x)).
\]

\medskip
\textbf{β–reduction of } \(B\,x\).
\[
B\,x
=
(\lambda g.\,\lambda y.\, g(g(g\,y)))\,x
\;\to_\beta\;
\lambda y.\, x\bigl(x(x\,y)\bigr).
\]
Name this \(B_x \equiv \lambda y.\, x^3(y)\), where \(x^k\) denotes \(k\)-fold self-composition.

\medskip
\textbf{β–reduction of } \(B\,(B_x)\).
\[
B\,(B_x)
=
(\lambda g.\,\lambda y.\, g(g(g\,y)))\,(\lambda y.\,x^3(y))
\;\to_\beta\;
\lambda y.\, B_x\bigl(B_x\,(B_x\,y)\bigr).
\]
Since \(B_x\,y = x^3(y)\), we compute
\[
B_x\bigl(B_x\,y\bigr) = B_x\bigl(x^3(y)\bigr) = x^3\!\bigl(x^3(y)\bigr) = x^6(y),
\]
and then
\[
B_x\bigl(B_x\,(B_x\,y)\bigr)
= B_x\bigl(x^6(y)\bigr)
= x^3\!\bigl(x^6(y)\bigr)
= x^9(y).
\]
Hence
\[
B\,(B_x) \;\to\; \lambda y.\, x^9(y).
\]

\medskip
\textbf{Assemble the result.}
\[
\lambda x.\, B\,(B\,x)
\;\to\;
\lambda x.\,\lambda y.\, x^9(y).
\]
Renaming \(x\mapsto f\) and \(y\mapsto a\) gives the canonical Church-numeral form
\[
\boxed{\lambda f.\,\lambda a.\, f^9(a)}.
\]
Thus
\[
(\lambda f.\,\lambda x.\, f(f\,x))\;(\lambda f.\,\lambda x.\, f(f(f\,x)))
\;=\;
\lambda f.\,\lambda x.\, f^9(x).
\]

\bigskip
\textbf{Remark (mathematical interpretation).}
In Church encodings, natural numbers are functions that iterate an endofunction:
\[
n \;\equiv\; \lambda f.\,\lambda x.\, f^{\,n}(x).
\]
Here,
\(
\lambda f.\lambda x.\, f(f\,x)
\)
is the numeral \(2\) (“apply \(f\) twice”) and
\(
\lambda f.\lambda x.\, f(f(f\,x))
\)
is the numeral \(3\) (“apply \(f\) thrice”).
Applying one numeral to another composes iterations and realizes \emph{exponentiation by iteration}:
\[
(2\;3) \;=\; \lambda x.\, 3^{\,2}(x) \;=\; \lambda f.\lambda x.\, f^{9}(x),
\]
so the result corresponds to the number \(9 = 3^2\).
Conceptually, this is \emph{function iteration}: “do thrice, then do thrice again” \(=\) “do nine times.”
Before computers, Church’s lambda calculus provided a purely symbolic foundation where numbers are actions (iterations) on functions; your reduction is exactly the arithmetic law that composing “apply-thrice” with itself yields “apply-nine-times.”


\subsubsection{Questions}

Since Church numerals let us represent arithmetic entirely through function application, what does that suggest about the nature of mathematics—are operations like addition, multiplication, and exponentiation really ‘just’ repeated patterns of substitution and iteration?

\subsection{Week 6}

\subsubsection{Homework}

\section*{Evaluating \texorpdfstring{$\mathtt{fact}\;3$}{fact 3} with \texorpdfstring{$\mathtt{fix}$}{fix}, \texorpdfstring{$\mathtt{let}$}{let}, and \texorpdfstring{$\mathtt{let\ rec}$}{let rec}}

\noindent\textbf{Rules}
\[
\begin{aligned}
\mathtt{fix}\;F &\;\to\; F\,(\mathtt{fix}\;F) &&\text{(def of $\mathtt{fix}$)}\\[2pt]
\mathtt{let}\;x = e_1\;\mathtt{in}\;e_2 &\;\to\; (\lambda x.\,e_2)\,e_1 &&\text{(def of $\mathtt{let}$)}\\[2pt]
\mathtt{let\ rec}\;f = e_1\;\mathtt{in}\;e_2 &\;\to\; \mathtt{let}\;f = \mathtt{fix}\,(\lambda f.\,e_1)\;\mathtt{in}\;e_2 
&&\text{(def of $\mathtt{let\ rec}$)}
\end{aligned}
\]

\noindent\textbf{Abbreviation}\quad
\(F \;\equiv\; \lambda f.\,\lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * f\,(n-1).\)

\medskip

\noindent\textbf{Target term}
\[
\mathtt{let\ rec}\; \mathtt{fact} = \lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * \mathtt{fact}\,(n-1)\; \mathtt{in}\; \mathtt{fact}\;3.
\]

\noindent\textbf{Derivation (each step labeled)}

\begingroup
\allowdisplaybreaks
\[
\begin{aligned}
&\mathtt{let\ rec}\; \mathtt{fact} = \lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * \mathtt{fact}\,(n-1)\; \mathtt{in}\; \mathtt{fact}\;3\\
%
&\xrightarrow{\text{def of let rec}}
\mathtt{let}\; \mathtt{fact} = \mathtt{fix}\,(\lambda \mathtt{fact}.\;\lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * \mathtt{fact}\,(n-1))\; \mathtt{in}\; \mathtt{fact}\;3\\
%
&\xrightarrow{\text{def of let}}
(\lambda \mathtt{fact}.\; \mathtt{fact}\;3)\; \mathtt{fix}\,(\lambda \mathtt{fact}.\;\lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * \mathtt{fact}\,(n-1))\\
%
&\xrightarrow{\beta}
\big(\mathtt{fix}\,(\lambda \mathtt{fact}.\;\lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * \mathtt{fact}\,(n-1))\big)\;3\\[2pt]
&= (\mathtt{fix}\;F)\;3\\
%
&\xrightarrow{\text{def of fix}}
\big(F\,(\mathtt{fix}\;F)\big)\;3\\
%
&\xrightarrow{\beta}
\big(\lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * (\mathtt{fix}\;F)\,(n-1)\big)\;3\\
%
&\xrightarrow{\beta}
\mathtt{if}\; 3=0\; \mathtt{then}\; 1\; \mathtt{else}\; 3 * (\mathtt{fix}\;F)\,2\\
%
&\xrightarrow{\text{def of if}}
3 * (\mathtt{fix}\;F)\,2
\end{aligned}
\]
\endgroup

\noindent Expand $(\mathtt{fix}\;F)\,2$:

\[
\begin{aligned}
(\mathtt{fix}\;F)\,2
&\xrightarrow{\text{def of fix}} \big(F\,(\mathtt{fix}\;F)\big)\,2
\xrightarrow{\beta} \big(\lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * (\mathtt{fix}\;F)\,(n-1)\big)\;2\\
&\xrightarrow{\beta} \mathtt{if}\; 2=0\; \mathtt{then}\; 1\; \mathtt{else}\; 2 * (\mathtt{fix}\;F)\,1
\xrightarrow{\text{def of if}} 2 * (\mathtt{fix}\;F)\,1.
\end{aligned}
\]

\noindent Expand $(\mathtt{fix}\;F)\,1$:

\[
\begin{aligned}
(\mathtt{fix}\;F)\,1
&\xrightarrow{\text{def of fix}} \big(F\,(\mathtt{fix}\;F)\big)\,1
\xrightarrow{\beta} \big(\lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * (\mathtt{fix}\;F)\,(n-1)\big)\;1\\
&\xrightarrow{\beta} \mathtt{if}\; 1=0\; \mathtt{then}\; 1\; \mathtt{else}\; 1 * (\mathtt{fix}\;F)\,0
\xrightarrow{\text{def of if}} 1 * (\mathtt{fix}\;F)\,0.
\end{aligned}
\]

\noindent Base case $(\mathtt{fix}\;F)\,0$:

\[
\begin{aligned}
(\mathtt{fix}\;F)\,0
&\xrightarrow{\text{def of fix}} \big(F\,(\mathtt{fix}\;F)\big)\,0
\xrightarrow{\beta} \big(\lambda n.\; \mathtt{if}\; n=0\; \mathtt{then}\; 1\; \mathtt{else}\; n * (\mathtt{fix}\;F)\,(n-1)\big)\;0\\
&\xrightarrow{\beta} \mathtt{if}\; 0=0\; \mathtt{then}\; 1\; \mathtt{else}\; 0 * (\mathtt{fix}\;F)\,(-1)
\xrightarrow{\text{def of if}} 1.
\end{aligned}
\]

\noindent Back-substitute the arithmetic:
\[
(\mathtt{fix}\;F)\,0 = 1,\quad
(\mathtt{fix}\;F)\,1 = 1,\quad
(\mathtt{fix}\;F)\,2 = 2,\quad
(\mathtt{fix}\;F)\,3 = \boxed{6}.
\]



\subsubsection{Questions}

If the $\mathtt{fix}$ operator allows a function to ``refer to itself'' without any explicit naming or looping construct, what does that reveal about recursion as a \emph{mathematical} idea rather than merely a \emph{programming} one? 

Does this suggest that self-reference---and therefore recursion---emerges purely from the rules of substitution and function application, rather than from syntactic features like \texttt{while} or \texttt{for} loops?

\subsection{Week 7}

\subsubsection{Homework}

\newcommand{\tok}[1]{\texttt{#1}}

\section*{Parse Trees for the Given Grammar}

\noindent\textbf{Grammar}
\[
\begin{aligned}
\mathrm{Exp}  &\to \mathrm{Exp}\;{+}\;\mathrm{Exp1}\;\mid\;\mathrm{Exp1}\\
\mathrm{Exp1} &\to \mathrm{Exp1}\;{*}\;\mathrm{Exp2}\;\mid\;\mathrm{Exp2}\\
\mathrm{Exp2} &\to \mathrm{Integer}\;\mid\;(\,\mathrm{Exp}\,)
\end{aligned}
\]

\subsection*{1) \tok{2+1}}
\resizebox{\linewidth}{!}{%
\Tree
[.Exp
  [.Exp [.Exp1 [.Exp2 [.Integer \tok{2} ] ] ] ]
  \tok{+}
  [.Exp1 [.Exp2 [.Integer \tok{1} ] ] ]
]}

\subsection*{2) \tok{1+2*3}}
\resizebox{\linewidth}{!}{%
\Tree
[.Exp
  [.Exp [.Exp1 [.Exp2 [.Integer \tok{1} ] ] ] ]
  \tok{+}
  [.Exp1
     [.Exp1 [.Exp2 [.Integer \tok{2} ] ] ]
     \tok{*}
     [.Exp2 [.Integer \tok{3} ] ]
  ]
]}

\subsection*{3) \tok{1+(2*3)}}
\resizebox{\linewidth}{!}{%
\Tree
[.Exp
  [.Exp [.Exp1 [.Exp2 [.Integer \tok{1} ] ] ] ]
  \tok{+}
  [.Exp1
    [.Exp2
      \tok{(}
      [.Exp
        [.Exp1
          [.Exp1 [.Exp2 [.Integer \tok{2} ] ] ]
          \tok{*}
          [.Exp2 [.Integer \tok{3} ] ]
        ]
      ]
      \tok{)}
    ]
  ]
]}

\subsection*{4) \tok{(1+2)*3}}
\resizebox{\linewidth}{!}{%
\Tree
[.Exp
  [.Exp1
    [.Exp1
      [.Exp2
        \tok{(}
        [.Exp
          [.Exp
            [.Exp1 [.Exp2 [.Integer \tok{1} ] ] ]
          ]
          \tok{+}
          [.Exp1 [.Exp2 [.Integer \tok{2} ] ] ]
        ]
        \tok{)}
      ]
    ]
    \tok{*}
    [.Exp2 [.Integer \tok{3} ] ]
  ]
]}

\subsection*{5) \tok{1+2*3+4*5+6}}
\resizebox{\linewidth}{!}{%
\Tree
[.Exp
  [.Exp
    [.Exp
      [.Exp
        [.Exp1 [.Exp2 [.Integer \tok{1} ] ] ]
      ]
      \tok{+}
      [.Exp1
        [.Exp1 [.Exp2 [.Integer \tok{2} ] ] ]
        \tok{*}
        [.Exp2 [.Integer \tok{3} ] ]
      ]
    ]
    \tok{+}
    [.Exp1
      [.Exp1 [.Exp2 [.Integer \tok{4} ] ] ]
      \tok{*}
      [.Exp2 [.Integer \tok{5} ] ]
    ]
  ]
  \tok{+}
  [.Exp1 [.Exp2 [.Integer \tok{6} ] ] ]
]}

\subsubsection{Questions}
When a grammar allows multiple ways to parse the same expression, such as different groupings of additions and multiplications, how can we modify or design the grammar so that the parse tree always reflects the correct operator precedence and associativity?

\subsection{Week 8}

\subsubsection{Homework}

\section*{Level 5/8: \textit{Adding zero}}

\paragraph{Question.}
Prove, for natural numbers \(a,b,c\),
\[
a + (b + 0) + (c + 0) \;=\; a + b + c.
\]

\begin{solution}
Using the identity \(x + 0 = x\) (often named \(\texttt{add\_zero}\)) twice:
\[
\begin{aligned}
a + (b + 0) + (c + 0)
&= a + b + (c + 0) &&\text{(by } b+0=b\text{)}\\
&= a + b + c       &&\text{(by } c+0=c\text{).}
\end{aligned}
\]
\medskip
\noindent\textbf{Lean script.}
\begin{lstlisting}
-- Objects: a b c : ℕ
rw [add_zero]      -- a + (b + 0) + (c + 0)  ==>  a + b + (c + 0)
rw [add_zero]      -- a + b + (c + 0)       ==>  a + b + c
rfl                -- both sides now identical
\end{lstlisting}
\end{solution}

\bigskip

\section*{Level 6/8: \textit{Precision rewriting}}

\paragraph{Question.}
Prove, for natural numbers \(a,b,c\),
\[
a + (b + 0) + (c + 0) \;=\; a + b + c,
\]
but emphasize \emph{targeted} rewriting (i.e., rewriting only the needed subterm each step).

\begin{solution}
Same identities as above, but apply them precisely to the subterm containing \(0\):
\[
\begin{aligned}
a + (b + 0) + (c + 0)
&= a + b + (c + 0) &&\text{rewrite only the inner }(b+0)\\
&= a + b + c       &&\text{now rewrite the remaining }(c+0).
\end{aligned}
\]
\medskip
\noindent\textbf{Lean script (one precise rewrite at a time).}
\begin{lstlisting}
-- Objects: a b c : ℕ
rw [add_zero]      -- rewrite (b + 0) -> b
rw [add_zero]      -- rewrite (c + 0) -> c
rfl
\end{lstlisting}
(If the goal were more complicated, one could direct a rewrite to a specific occurrence using a location, e.g. `rw [add_zero] at h` or `simp [add_zero]` with side conditions.)
\end{solution}

\bigskip

\section*{Level 7/8: \textit{add\_succ}}

\paragraph{Theorem (stated).}
\[
\forall n:\mathbb{N},\quad n.\mathrm{succ} \;=\; n + 1.
\]

\paragraph{Useful lemmas.}
\[
\begin{aligned}
1 &= \mathrm{succ}(0) &&\text{(named \texttt{one\_eq\_succ\_zero})},\\
n + \mathrm{succ}(k) &= \mathrm{succ}(n + k) &&\text{(named \texttt{add\_succ})},\\
n + 0 &= n &&\text{(named \texttt{add\_zero}).}
\end{aligned}
\]

\begin{solution}
\[
\begin{aligned}
n.\mathrm{succ}
&= n + 0.\mathrm{succ} &&\text{(since } \mathrm{succ}(x) = x+1 \text{, we move via }1=\mathrm{succ}(0))\\
&= \mathrm{succ}(n + 0) &&\text{by } \texttt{add\_succ}\\
&= \mathrm{succ}(n) &&\text{by } \texttt{add\_zero}\\
&= n + 1 &&\text{unwinding } 1=\mathrm{succ}(0).
\end{aligned}
\]
Concretely (mirroring your screenshot), we first rewrite the \(1\) as \(\mathrm{succ}(0)\), then use \(\texttt{add\_succ}\), then \(\texttt{add\_zero}\), and finish with reflexivity.
\medskip

\noindent\textbf{Lean script (exact sequence from the level).}
\begin{lstlisting}
-- Goal: n.succ = n + 1
rw [one_eq_succ_zero]   -- turn 1 into succ 0 on the RHS
rw [add_succ]           -- n + succ 0  ->  (n + 0).succ
rw [add_zero]           -- (n + 0).succ -> n.succ
rfl                     -- both sides are n.succ
\end{lstlisting}
\end{solution}

\bigskip\bigskip
\noindent\emph{Notes.}
\begin{itemize}
  \item Levels 5 and 6 highlight the same identity \(x+0=x\) but train you to control \emph{where} a rewrite happens.
  \item Level 7 chains small lemmas to reach a familiar arithmetic fact: \(n+1=\mathrm{succ}(n)\).
\end{itemize}

\section*{Question 8 — Natural–Language Proof (Level 8: $2+2=4$)}

\paragraph{Goal.} Prove that $2+2=4$ in Peano arithmetic.

\paragraph{Background/Definitions.}
Let $0$ be the base natural number and let $\mathrm{succ}(n)$ (sometimes written $n.succ$) denote the successor of $n$.
Define the numerals
\[
1 \;=\; \mathrm{succ}(0),\qquad
2 \;=\; \mathrm{succ}(1),\qquad
3 \;=\; \mathrm{succ}(2),\qquad
4 \;=\; \mathrm{succ}(3).
\]
Addition is defined by the standard recursion rules:
\[
\text{(add\_zero)}\quad n+0=n,
\qquad
\text{(add\_succ)}\quad n+\mathrm{succ}(k)=\mathrm{succ}(n+k).
\]

\paragraph{Proof.}
We compute $2+2$ using only the recursion rules for $+$ and the numeral definitions.

\begin{align*}
2+2
&= \mathrm{succ}(1) + \mathrm{succ}(1) && \text{(by the definition of $2$)}\\
&= \mathrm{succ}\big(\,\mathrm{succ}(1) + 1\,\big) && \text{(by \textit{add\_succ} with $n=\mathrm{succ}(1),\,k=1$)}\\
&= \mathrm{succ}\big(\,\mathrm{succ}(1) + \mathrm{succ}(0)\,\big) && \text{(since $1=\mathrm{succ}(0)$)}\\
&= \mathrm{succ}\Big(\,\mathrm{succ}\big(\,\mathrm{succ}(1) + 0\,\big)\Big) && \text{(by \textit{add\_succ} with $n=\mathrm{succ}(1),\,k=0$)}\\
&= \mathrm{succ}\Big(\,\mathrm{succ}\big(\,\mathrm{succ}(1)\,\big)\Big) && \text{(by \textit{add\_zero}: $\mathrm{succ}(1)+0=\mathrm{succ}(1)$)}\\
&= \mathrm{succ}\big(\,\mathrm{succ}(\mathrm{succ}(1))\,\big)\\
&= \mathrm{succ}\big(\,\mathrm{succ}(\mathrm{succ}(\mathrm{succ}(0)))\,\big) && \text{(since $1=\mathrm{succ}(0)$)}\\
&= \mathrm{succ}(\mathrm{succ}(\mathrm{succ}(\mathrm{succ}(0)))))\\
&= 4 && \text{(by the definition of $4$).}
\end{align*}

Therefore, $2+2=4$.
\qed

\paragraph{(Optional) Lean step correspondence.}
The proof above mirrors the scripted steps:
\[
\texttt{rw [four\_eq\_succ\_three]\; rw [three\_eq\_succ\_two]\; rw [two\_eq\_succ\_one]\; rw [one\_eq\_succ\_zero]\; rw [add\_succ]\; rw [add\_zero]\; rfl.}
\]

\subsubsection{Questions}
In Peano arithmetic, every arithmetic fact—such as $2+2=4$—is proven step by step from simple definitions like $0$ and $\mathrm{succ}(n)$. 
What does this tell us about how much of mathematics can be built from just a few basic rules, and why might this level of formality be both powerful and limiting when compared to the way we usually do arithmetic?

\subsection{Week 9}

\subsubsection{Homework}

Here’s a compile-ready LaTeX block you can drop into your Week 9 report. It contains **two solutions** for Addition World – **Level 5: (\mathsf{add_right_comm})**: one by **induction** (spelled out in English math) and one **without induction** (a short proof using the lemma `add_right_comm`). I’ve also included the Lean scripts that match each proof.

```latex
\section*{Addition World — Level 5: \texorpdfstring{$\mathsf{add\_right\_comm}$}{add\_right\_comm}}

\paragraph{Theorem (goal).}
For all natural numbers \(a,b,c\),
\[
a + b + c \;=\; a + c + b .
\]

\subsection*{Solution A: Inductive proof on \texorpdfstring{$b$}{b} (no tactics required to understand)}
We use the Peano rules for addition:
\[
\text{(add\_zero)}\; n+0=n,\qquad
\text{(add\_succ)}\; n+\operatorname{succ}(k)=\operatorname{succ}(n+k),
\]
together with associativity \((x+y)+z=x+(y+z)\) and commutativity \(x+y=y+x\).

\paragraph{Claim.} For all \(a,c\), \(P(b):\; a+b+c=a+c+b\) holds by induction on \(b\).

\emph{Base case \(b=0\).}
\[
a+0+c \;=\; a+c \;=\; a+c+0,
\]
by two uses of \(\text{add\_zero}\). Hence \(P(0)\) holds.

\emph{Inductive step.} Assume \(P(b)\) holds, i.e.
\[
a+b+c \;=\; a+c+b
\quad\text{for all }a,c.
\]
We must show \(P(\operatorname{succ} b)\):
\[
a+\operatorname{succ}b+c \;=\; a+c+\operatorname{succ}b.
\]
Compute:
\[
\begin{aligned}
a+\operatorname{succ}b+c
&= (a+\operatorname{succ}b)+c\\
&= \operatorname{succ}(a+b)+c &&\text{(by \(\text{add\_succ}\))}\\
&= \operatorname{succ}\big((a+b)+c\big) &&\text{(by \(\text{succ\_add}\): }\operatorname{succ}x+c=\operatorname{succ}(x+c))\\
&= \operatorname{succ}(a+b+c) &&\text{(associativity)}\\
&= \operatorname{succ}(a+c+b) &&\text{(induction hypothesis)}\\
&= a+c+\operatorname{succ}b &&\text{(reverse of \(\text{add\_succ}\)).}
\end{aligned}
\]
Thus \(P(\operatorname{succ}b)\) holds, and by induction \(a+b+c=a+c+b\) for all \(a,b,c\).

\subsection*{Solution B: Direct proof \emph{without} induction (one-line rewrite)}
If the library lemma \(\mathsf{add\_right\_comm}\) is available, it states exactly
\[
x + y + z \;=\; x + z + y .
\]
Therefore, taking \(x=a\), \(y=b\), \(z=c\) gives the result immediately:
\[
a + b + c \;=\; a + c + b.
\]


\subsubsection{Questions}
Why do we need induction to prove some arithmetic properties like commutativity or associativity, but not others? 
In other words, what makes a property like $a + b + c = a + c + b$ provable directly from existing lemmas, 
while others require an inductive step on one of the variables?

\subsection{Week 10}

\subsubsection{Homework}
\section*{Lean Logic Game — Party Snacks (Levels 6–9, one line each)}

\subsection*{Level 6: \texttt{and\_imp}}

\noindent\textbf{One-liner:}
\begin{lstlisting}
exact (fun c => fun d => h (And.intro c d))
-- equivalently: exact fun c d => h ⟨c, d⟩
\end{lstlisting}

\subsection*{Level 7: \texttt{and\_imp 2}}

\noindent\textbf{One-liner:}
\begin{lstlisting}
exact (fun hd => h hd.left hd.right)
-- equivalently: exact fun ⟨hc, hd⟩ => h hc hd
\end{lstlisting}

\subsection*{Level 8: \texttt{Distribute}}

\noindent\textbf{One-liner:}
\begin{lstlisting}
exact fun s => And.intro (h.left s) (h.right s)
-- equivalently: exact fun s => ⟨h.left s, h.right s⟩
\end{lstlisting}

\subsection*{Level 9: \texttt{Uncertain Snacks}}

\noindent\textbf{One-liner:}
\begin{lstlisting}
exact fun r => And.intro (fun _ : S => r) (fun _ : ¬ S => r)
-- equivalently: exact fun r => ((fun _ : S => r), (fun _ : ¬ S => r))
\end{lstlisting}

\subsubsection{Questions}
How does writing logical proofs as functions in Lean change the way we think about reasoning with implications? 
Does it make the structure of logical arguments clearer compared to traditional symbolic proofs on paper?

\subsection{Week 11}

\subsubsection{Homework}

\section*{Lean Logic Game — Negation (Levels 9–12, one line each)}

\subsection*{Level 9 / 12: Implies a Negation}
\textbf{Problem.}
\[
\text{example }(A\,P:\mathrm{Prop})\,(h:P\to \lnot A):\ \lnot(P\land A)
\]
\textbf{One-line solution (Lean).}
\begin{lstlisting}
exact fun (p, a) => (h p) a
\end{lstlisting}

\subsection*{Level 10 / 12: Conjunction Implication}
\textbf{Problem.}
\[
\text{example }(A\,P:\mathrm{Prop})\,(h:\lnot(P\land A)):\ P\to \lnot A
\]
\textbf{One-line solution (Lean).}
\begin{lstlisting}
exact fun p a => h (p, a)
\end{lstlisting}

\subsection*{Level 11 / 12: \texttt{not\_not\_not}}
\textbf{Problem.}
\[
\text{example }(A:\mathrm{Prop})\,(h:\lnot\lnot\lnot A):\ \lnot A
\]
\textbf{One-line solution (Lean).}
\begin{lstlisting}
exact fun a => h (fun na => na a)
\end{lstlisting}


\subsection*{Level 12 / 12: \texttt{\textasciitilde Intro Boss}}
\textbf{Problem.}
\[
\text{example }(B\,C:\mathrm{Prop})\,(h:\lnot(B+C)):\ \lnot\lnot B
\]
\textbf{One-line solution (Lean).}
\begin{lstlisting}
exact fun nB => h (fun b => False.elim (nB b))
\end{lstlisting}

\subsubsection{Questions}
How does encoding \(\lnot A\) as \(A\to \mathrm{False}\) help us construct one-line proofs using only function application? 
Does this viewpoint make the structure of negation proofs clearer than traditional derivations?

\subsection{Week 12}

\subsubsection{Homework}

\section*{Towers of Hanoi (2025) — Activity Write-up (n = 5, from peg 0 to peg 2)}

\subsection*{Algorithm (rules)}
\[
\begin{aligned}
\textsf{hanoi}\;1\;x\;y &\;=\; \textsf{move}\;x\;y\\
\textsf{hanoi}\;(n{+}1)\;x\;y &\;=\;
  \textsf{hanoi}\;n\;x\;(\textsf{other}\;x\;y)\,;\;
  \textsf{move}\;x\;y\,;\;
  \textsf{hanoi}\;n\;(\textsf{other}\;x\;y)\;y
\end{aligned}
\]

\subsection*{(1) Completed execution trace for \textsf{hanoi} 5 0 2}
\small
\begin{verbatim}
hanoi 5 0 2
    hanoi 4 0 1
        hanoi 3 0 2
            hanoi 2 0 1
                hanoi 1 0 2 = move 0 2
                move  0 1
                hanoi 1 2 1 = move 2 1
            move  0 2
            hanoi 2 1 2
                hanoi 1 1 0 = move 1 0
                move  1 2
                hanoi 1 0 2 = move 0 2
        move  0 1
        hanoi 3 2 1
            hanoi 2 2 0
                hanoi 1 2 1 = move 2 1
                move  2 0
                hanoi 1 1 0 = move 1 0
            move  2 1
            hanoi 2 0 1
                hanoi 1 0 2 = move 0 2
                move  0 1
                hanoi 1 2 1 = move 2 1
    move  0 2
    hanoi 4 1 2
        hanoi 3 1 0
            hanoi 2 1 2
                hanoi 1 1 0 = move 1 0
                move  1 2
                hanoi 1 0 2 = move 0 2
            move  1 0
            hanoi 2 2 0
                hanoi 1 2 1 = move 2 1
                move  2 0
                hanoi 1 1 0 = move 1 0
        move  1 2
        hanoi 3 0 2
            hanoi 2 0 1
                hanoi 1 0 2 = move 0 2
                move  0 1
                hanoi 1 2 1 = move 2 1
            move  0 2
            hanoi 2 1 2
                hanoi 1 1 0 = move 1 0
                move  1 2
                hanoi 1 0 2 = move 0 2
\end{verbatim}
\normalsize

\subsection*{(2) Moves extracted from the trace (in order)}
Each line \texttt{move x y} is a move of the top disk from peg \(x\) to peg \(y\).  
There are \(2^5{-}1=31\) moves, the minimal number for \(n{=}5\).

\[
\begin{aligned}
&(0\!\to\!2),\ (0\!\to\!1),\ (2\!\to\!1),\ (0\!\to\!2),\ (1\!\to\!0),\ (1\!\to\!2),\ (0\!\to\!2),\\
&(0\!\to\!1),\ (2\!\to\!1),\ (2\!\to\!0),\ (1\!\to\!0),\ (2\!\to\!1),\ (0\!\to\!2),\ (0\!\to\!1),\ (2\!\to\!1),\\
&(0\!\to\!2),\\
&(1\!\to\!0),\ (1\!\to\!2),\ (0\!\to\!2),\ (1\!\to\!0),\ (2\!\to\!1),\ (2\!\to\!0),\ (1\!\to\!0),\\
&(1\!\to\!2),\\
&(0\!\to\!2),\ (0\!\to\!1),\ (2\!\to\!1),\ (0\!\to\!2),\ (1\!\to\!0),\ (1\!\to\!2),\ (0\!\to\!2).
\end{aligned}
\]

\subsection*{(3) How to verify against the online Towers of Hanoi}
\begin{enumerate}[leftmargin=1.3em]
  \item Open the online Towers of Hanoi and set \(n=5\), source peg \(0\), target peg \(2\).
  \item Play the moves above in order; every move is legal (never places a larger disk on a smaller one).
  \item You will finish in exactly \(31\) moves, which is minimal for \(n=5\).
  \item (Why this works.) The algorithm preserves the invariant “the top \(k\) disks are always in legal configuration”, and the recursive pattern
    \[
      \textsf{hanoi}\;n\;x\;y = 
      \textsf{hanoi}\;(n{-}1)\;x\;z;\ \textsf{move}\;x\;y;\ \textsf{hanoi}\;(n{-}1)\;z\;y
    \]
    ensures inductively that the largest disk moves exactly once (midpoint move), surrounded by optimal solutions for size \(n{-}1\).
\end{enumerate}

\subsubsection{Questions}
Why does the recursive algorithm for the Towers of Hanoi always produce the minimum possible number of moves, and how does seeing the full execution trace help you understand why no shorter solution could exist?

\subsection{Week 13}

\subsubsection{Homework}

\section*{Item 2: Testing the Interpreter}

\subsection*{2.1 Does the implementation conform to the theory?}

Running \texttt{python interpreter\_test.py} executes a battery of tests that check
beta-reduction, alpha-conversion, and the chosen evaluation strategy. All the
provided tests pass, so the implementation behaves as specified by the
mathematical theory of the untyped lambda calculus (in particular: correct
handling of bound vs.\ free variables and correct sequencing of reductions).

\subsection*{2.2 Additional test cases}

I added several expressions inspired by the lecture notes to \texttt{test.lc},
always predicting the expected normal form before running the interpreter:

\begin{itemize}
  \item \(\;a\;b\;c\;d\)

  By definition, application in the lambda calculus is left-associative, so
  \[
    a\;b\;c\;d \equiv (((a\;b)\;c)\;d).
  \]
  The interpreter prints exactly this left-nested structure.

  \item \((a)\)

  Parentheses do not change the term, so \((a)\) and \(a\) are alpha-identical.
  There is no beta-redex, so \((a)\) is already in normal form and reduces to
  \(a\).

  \item Church booleans
  \[
    \mathtt{true} \;\equiv\; \lam t.\lam f.\,t
    \qquad
    \mathtt{false} \;\equiv\; \lam t.\lam f.\,f
  \]
  and the conditional
  \[
    \mathtt{if} \;\equiv\; \lam b.\lam x.\lam y.\,b\;x\;y.
  \]
  For example
  \[
    \mathtt{if}\;\mathtt{true}\;M\;N
    \;\Rightarrow_\beta^*\;
    M,
  \]
  and
  \[
    \mathtt{if}\;\mathtt{false}\;M\;N
    \;\Rightarrow_\beta^*\;
    N.
  \]
  The interpreter returns the expected branch in both cases.

  \item Church numerals
  \[
    \mathbf{0} \equiv \lam f.\lam x.\,x,\quad
    \mathbf{1} \equiv \lam f.\lam x.\,f\,x,\quad
    \mathbf{2} \equiv \lam f.\lam x.\,f(fx),\quad
    \mathbf{3} \equiv \lam f.\lam x.\,f(f(fx)).
  \]

  From HW5 we know that
  \[
    (\lam f.\lam x.\,f(fx))\;(\lam f.\lam x.\,f(f(fx)))
  \]
  should reduce to the Church numeral for \(9\), i.e.
  \[
    \mathbf{9} \equiv \lam f.\lam x.\,\underbrace{f(f(\dots f}_{9\text{ times}}x\dots)).
  \]
  The interpreter indeed normalizes this term to a lambda-term of the form
  \(\lam f.\lam x.\,f^9 x\).
\end{itemize}

These experiments support the claim that the implementation conforms to the
intended operational semantics (normal-order beta-reduction with
alpha-conversion).

\section*{Item 3: Capture-avoiding substitution}

Substitution is written \([x := N]M\), meaning “replace all \emph{free}
occurrences of \(x\) in \(M\) by \(N\), avoiding variable capture”. The
mathematical definition is by structural recursion on \(M\).

\begin{enumerate}
  \item Variables:
  \[
    [x := N]x \;=\; N
    \qquad
    [x := N]y \;=\; y \quad \text{if } y \neq x.
  \]

  \item Application:
  \[
    [x := N](M_1\,M_2)
    \;=\;
    ([x := N]M_1)\;([x := N]M_2).
  \]

  \item Abstraction: there are three cases.
  \[
    [x := N](\lam y.M) \;=\;
    \begin{cases}
      \lam y.M, & \text{if } y = x, \\[4pt]
      \lam y.[x := N]M, & \text{if } y \neq x \text{ and } y \notin \mathrm{FV}(N), \\[4pt]
      \lam z.\,[x := N]\,M[y := z], & \text{if } y \neq x \text{ and } y \in \mathrm{FV}(N),
    \end{cases}
  \]
  where \(z\) is a fresh variable not occurring free in \(M\) or \(N\) and
  \(M[y := z]\) is a \emph{capture-avoiding} renaming of the bound variable
  \(y\) to \(z\).

  The third case is where alpha-conversion happens: before substituting, we
  rename the binder \(\lam y\) to a fresh \(\lam z\) so that the free
  variables of \(N\) never accidentally become bound.
\end{enumerate}

\noindent
\textbf{Implementation sketch.}
In \texttt{interpreter.py} this is represented as a recursive
\texttt{substitute(var, repl, term)} function, where \texttt{term} is an AST
node of one of three kinds: variable, abstraction, or application. When
substitution enters an abstraction \(\lam y.M\) and the bound variable
\texttt{y} is in the free variables of \texttt{repl}, the interpreter first
generates a fresh name like \texttt{Var1}, \texttt{Var2}, \dots, renames the
binder and its bound occurrences (alpha-conversion), and only then recurses.
This is exactly the algorithm described above.

\section*{Item 4: Do all computations reduce to normal form?}

For all “well-behaved’’ examples from the lectures (Church booleans, numerals,
pairs, lists, etc.) the interpreter produced the expected normal forms. In
particular, finite numeric computations and conditionals always normalized.

However, the untyped lambda calculus is Turing-complete, so there exist terms
with no normal form. For such terms the interpreter keeps performing beta
reductions forever (or, in practice, runs until we stop it), and no normal
form is reached. Thus:

\begin{itemize}
  \item For all test cases encoding finite computations, the interpreter gives
        the expected result.
  \item Not all lambda-terms reduce to normal form; some diverge.
\end{itemize}

\section*{Item 5: Smallest non-normalizing $\lambda$-expression}

A standard minimal working example (MWE) of a non-normalizing term is
\[
  \Omega \;\equiv\; (\lam x.\,x\,x)\;(\lam x.\,x\,x).
\]

\noindent
\textbf{Reduction:}
\[
  (\lam x.\,x\,x)\;(\lam x.\,x\,x)
  \;\Rightarrow_\beta\;
  [x := \lam x.\,x\,x](x\,x)
  \;=\;
  (\lam x.\,x\,x)\;(\lam x.\,x\,x)
  \;=\;
  \Omega.
\]
So one beta-reduction step takes us back to the same term. Every proper
subterm of \(\Omega\) is either a variable or an abstraction and hence already
in normal form, so \(\Omega\) is a very small example of a term that does not
reduce to normal form.

\section*{Item 7: Substitution trace for \\ 
\(\bigl((\lam m.\lam n.\,m\,n)(\lam f.\lam x.\,f(fx))\bigr)(\lam f.\lam x.\,f(f(fx)))\)}

We want to mirror the interpreter’s beta-reduction steps, focusing on
substitution. Below, each line corresponds to a single substitution (beta-step);
for readability we omit some obvious alpha-renamings and keep variable names
distinct by convention.

\begin{align*}
 &\bigl((\lam m.\lam n.\,m\,n)\,(\lam f.\lam x.\,f(fx))\bigr)\,(\lam f.\lam x.\,f(f(fx))) \\[4pt]
 \Rightarrow_\beta\;& (\lam n.\,(\lam f.\lam x.\,f(fx))\,n)\,(\lam f.\lam x.\,f(f(fx)))
 && \text{substitute } m := (\lam f.\lam x.\,f(fx)) \\[4pt]
 \Rightarrow_\beta\;& (\lam n.\,\lam x.\,n(n x))\,(\lam f.\lam x.\,f(f(fx)))
 && \text{substitute } f := n \text{ in } \lam x.\,f(fx) \\[4pt]
 \Rightarrow_\beta\;& \lam x.\,(\lam f.\lam x.\,f(f(fx)))\bigl((\lam f.\lam x.\,f(f(fx)))\,x\bigr)
 && \text{substitute } n := (\lam f.\lam x.\,f(f(fx))) \\[4pt]
 \Rightarrow_\beta\;& \lam x.\,(\lam x'.\,(\lam f.\lam x.\,f(f(fx)))\,(f_1)) 
 && \text{(alpha-renaming of inner }x\text{ to }x') \\[-2pt]
 &\hspace{5em}\text{where } f_1 \text{ abbreviates } \lam x.\,f(f(fx)) \\[4pt]
 \Rightarrow_\beta\;& \lam f.\lam x.\,\underbrace{f(f(\dots f}_{9\ \text{times}}x\dots))
\end{align*}

The detailed middle steps just repeatedly expand the nested applications of
the Church numeral \(\mathbf{3}\) to itself; after fully unwinding, the body
contains exactly nine occurrences of \(f\), so the result is the Church
numeral \(\mathbf{9}\).

\section*{Item 8: Recursive evaluation trace for \\
\(\bigl((\lam m.\lam n.\,m\,n)(\lam f.\lam x.\,f(fx))\bigr)(\lam f.\lam x.\,f x)\)}

Here we sketch the call trace of the interpreter’s \texttt{evaluate} and
\texttt{substitute} functions, in the same style as the recursive trace of
\texttt{hanoi}. We write \(\ell_{12}, \ell_{39}, \dots\) as symbolic labels for
the source-line numbers where these calls occur; in your own trace you should
replace them with the actual line numbers from \texttt{interpreter.py}.

\begin{verbatim}
ℓ12: evaluate( ((\m.(\n.(m n))) (\f.(\x.(f (f x))))) (\f.(\x.(f x))) )
  ℓ39: evaluate( (\m.(\n.(m n))) (\f.(\x.(f (f x)))) )
    ℓ52: evaluate( \m.(\n.(m n)) )
    ℓ53: evaluate( \f.(\x.(f (f x))) )
    ℓ70: substitute( m := (\f.(\x.(f (f x)))) 
                     in  \n.(m n) )
      ℓ80: (possible alpha-conversion inside substitute)
    ℓ90: evaluate( \n.((\f.(\x.(f (f x)))) n) )
  ℓ39: evaluate( \f.(\x.(f x)) )
  ℓ70: substitute( n := (\f.(\x.(f x))) 
                   in  ((\f.(\x.(f (f x)))) n) )
    ℓ80: (substitute into application)
      ℓ85: substitute into function part (\f.(\x.(f (f x))))
      ℓ86: substitute into argument n
  ℓ95: evaluate( (\f.(\x.(f (f x)))) (\f.(\x.(f x))) )
    ℓ52: evaluate( \f.(\x.(f (f x))) )
    ℓ53: evaluate( \f.(\x.(f x)) )
    ℓ70: substitute( f := (\f.(\x.(f x))) 
                     in  \x.(f (f x)) )
      ℓ80: (alpha-conversion if needed)
    ℓ90: evaluate( \x.(g (g x)) )   -- where g = (\f.(\x.(f x)))
\end{verbatim}

Conceptually, this trace shows that:

\begin{itemize}
  \item The outermost call to \texttt{evaluate} sees an application, so it
        recursively evaluates the function and argument parts.
  \item For each beta-reduction, the interpreter calls \texttt{substitute}
        with the bound variable, the argument term, and the body of the
        abstraction, performing alpha-conversion where necessary.
  \item The recursive structure of the calls mirrors the structure of nested
        applications in the lambda-term, just as the recursive calls of
        \texttt{hanoi} mirror the recursive problem decomposition.
\end{itemize}

\subsubsection{Questions}
When experimenting with the interpreter, what is one example where the evaluation did not terminate in a normal form, and what does this tell you about the expressive power (and risks) of the untyped lambda calculus?


\section{Essay}

A unifying theme of this course is that \emph{programming languages are
mathematical objects}. From the MIU puzzle and abstract reduction systems
(ARSs), through measure functions and termination proofs, to the lambda
calculus, interpreters, and Lean, we repeatedly saw that programs can be
understood as symbolic structures governed by precise rules of
transformation. In this essay I focus on the lambda calculus and
operational semantics as a way to synthesize the material and to connect
it to the broader practice of software engineering.

The untyped lambda calculus, introduced by Church in the 1930s, is a tiny
language with just variables, function abstraction, and application, yet
it is computationally universal. It also forms the conceptual foundation
of modern functional languages such as Haskell and ML.\cite{mukund-lambda}
In the homework and the interpreter project, I experienced this directly:
Church numerals turned numbers into “iterate this function $n$ times,” and
boolean values became higher-order functions selecting a branch. What
initially looks like a curiosity of logic turns out to be a powerful way
to think about data and control flow in real languages.

Abstract reduction systems and the lambda calculus meet again in the idea
of \emph{operational semantics}. A small-step semantics defines a reduction
relation $e \to e'$ describing how a program takes one tiny computational
step at a time, while a big-step semantics relates an expression directly
to its final value.\cite{ray-opsem} In the course, this showed up in two
places: (1) drawing ARS graphs and proving properties such as confluence
and unique normal forms; and (2) implementing an evaluator that repeatedly
applies beta-reduction and capture-avoiding substitution. Seeing the same
ideas in both a pen-and-paper proof style and in Python code made it clear
that an interpreter is just “the semantics made executable.”

This perspective matters for software engineering. Real-world languages
are far more complex than the toy calculi in class, but they are still
ultimately defined by some kind of operational semantics. When we worry
about whether an optimization preserves behaviour, whether a refactoring
is semantics-preserving, or whether a compiler pass can introduce bugs,
we are implicitly appealing to these formal models. Concepts like
measure functions and termination arguments also scale up: they underlie
loop variants, termination checkers, and tools that verify that recursive
functions or protocols eventually make progress.

Personally, the most interesting and useful part of the course was seeing
the tight connection between proofs and programs. Proving termination of
Euclid’s algorithm or merge sort using a measure, then later reasoning
about non-terminating lambda terms such as $\Omega$, sharpened my
intuition about when code is guaranteed to finish and when it might loop
forever. Working in Lean made this even more concrete: logical rules,
rewriting, and function definitions all became different faces of the
same underlying notion of computation.

If I had to suggest improvements, they would be minor: perhaps a few more
worked examples that explicitly connect each new formal concept (like
confluence or fixed points) to a concrete bug or design decision in a
popular language or library. Overall, though, the course already does a
good job of showing that programming-language theory is not just abstract
math, but a practical toolkit for reasoning about software systems.


\section{Evidence of Participation}

\subsection{\textit{Machine Code Explained} (Computerphile)}

\textbf{Summary.}
In this video, Matt Godbolt explains what machine code really is and how it relates to the programs we write in high-level languages. He starts from the basic idea that a CPU ultimately only understands very small, fixed-size instructions encoded as binary numbers. Each instruction tells the processor to do something extremely simple, like move a value into a register, add two registers, or jump to another location in memory. Godbolt shows how these binary patterns are often written in hexadecimal for convenience, and how assembly language is just a human-readable layer on top of those numeric opcodes.

He then illustrates how a tiny program is laid out in memory as a sequence of instructions, and how the CPU steps through them one by one using the program counter. Along the way, he connects this to compilers: when we compile C or another high-level language, the compiler’s job is to translate our source into this low-level instruction sequence. The video emphasizes that many “mysterious” behaviors, like crashes, performance quirks, or odd jumps, make more sense once you understand what the machine is actually executing and how tightly constrained machine instructions really are.

\textbf{Discussion Question.}
Once you understand that machine code is just a long sequence of simple instructions, how does that change the way you think about debugging or reasoning about the correctness of programs written in a high-level language?

\noindent\textbf{Link:} \url{https://www.youtube.com/watch?v=8VsiYWW9r48}

\subsection{\textit{Behind Mobile Game Design} (Olga Lazar)}

\textbf{Summary.}
In this talk, Olga Lazar shares her journey into mobile game design and offers a grounded look at what the day-to-day work of a game designer actually involves. She explains how designers sit at the intersection of art, programming, and business: they create core mechanics, tune difficulty and progression, and think constantly about player motivation and retention. Rather than just “coming up with ideas,” a lot of her work is iterative balancing, looking at player data, tweaking numbers, adjusting level layouts, and refining the feel of the game through repeated playtesting. She also emphasizes communication: designers have to translate high-level concepts into concrete specs that artists and engineers can implement, then negotiate trade-offs when technical or time constraints appear. Along the way, she talks about building a career in game design, including the importance of a portfolio, showing prototypes rather than just ideas, and being willing to start small (e.g., in QA or junior roles) and grow into more responsibility.

\textbf{Discussion Question.}
Game designers often balance “what would be fun” with “what will keep players engaged and paying over time.” When those two goals conflict, how should a designer decide which side to prioritize, and what ethical limits should they place on their own design choices?

\noindent\textbf{Link:} \url{https://www.youtube.com/watch?v=VFkLXbSf7Co}

\subsection{\textit{Creating Your Own Programming Language} (Computerphile)}

\textbf{Summary.}
In the Computerphile video “Creating Your Own Programming Language,” Dr.~Laurie Tratt demonstrates how surprisingly little code is required to build a tiny, working programming language. Instead of starting with heavy theory, he takes a practical, incremental approach: first choosing a minimal syntax (reverse Polish notation) to dodge complex parsing, then writing a small interpreter that uses a stack to evaluate expressions. From there, he gradually adds features, variables stored in a dictionary, basic control flow, and looping constructs, showing that core language ideas (state, branching, iteration) can emerge from a very compact design. Throughout, he emphasizes that languages are just programs like any other: they are made of concrete implementation choices and trade-offs, not magic. The video highlights how building a toy language helps deepen understanding of real-world languages, interpreters, and compilers by making the underlying mechanisms visible and manipulable.

\textbf{Discussion Question.}
When designing a small language for learning, is it better to keep the syntax and features extremely minimal, or to include a few “real-world” conveniences even if they complicate the implementation?

\noindent\textbf{Link:} \url{https://www.youtube.com/watch?v=Q2UDHY5as90}

\subsection{Untyped Lambda Calculus Introduction}

\textbf{Summary.}
In this video, the instructor introduces the untyped lambda calculus as a tiny but surprisingly powerful “language of only functions.” He starts by explaining the basic syntax (variables, function abstractions, and applications) and then walks through how computation happens via substitution and step-by-step reduction (beta-reduction). Along the way he emphasizes free vs.\ bound variables and why we sometimes need to rename bound variables (alpha-conversion) to avoid accidental capture when substituting. With that foundation in place, he shows how everyday programming concepts emerge from pure functions: booleans as functions that choose between two arguments, logical operators like AND/OR built from those booleans, and natural numbers as Church numerals that represent “apply this function $n$ times.”

The video then builds up more complex behavior: arithmetic on Church numerals, conditionals, pairs, and finally recursion via a fixed-point combinator (a Y-combinator–style construction). By the end, you see how a language with no built-in numbers, booleans, loops, or variables in the usual sense can still express all the usual programming patterns. The instructor frequently connects these ideas back to functional programming, pointing out how modern languages (like Haskell and ML-style languages) are grounded in the same calculus. Overall, the video reinforces a major theme from the course: that programs can be modeled as mathematical objects and that “real” languages are elaborate skins over a small, well-defined core.

\textbf{Discussion Question.}
If the untyped lambda calculus can encode numbers, booleans, conditionals, and recursion using only functions, do you think that starting from such a tiny core makes it easier or harder to understand real-world programming languages later on?

\noindent\textbf{Link:} \url{https://www.youtube.com/watch?v=v5FBAbnLjUQ}

\subsection{\textit{Procedural Generation in Games}}

\textbf{Summary.}
In this video, Dr.~Mike Cook explains procedural generation in games, how simple rules and algorithms can automatically create huge, varied worlds and content instead of designers building everything by hand. He starts with the idea that procedural content is generated by code rather than directly authored, and shows how small rule sets can expand into large, rich spaces of possibilities. He discusses examples like terrain, levels, and items, and contrasts random generation with structured generation, where constraints, weights, and rules are used to keep results fun and playable instead of chaotic.

Dr.~Cook also talks about trade-offs: procedural systems can save time and create endless variety, but they can also feel repetitive or meaningless if the underlying rules are too shallow. Good procedural design requires thinking about player experience, what patterns players will notice, how difficulty flows, and how to avoid impossible or boring configurations. In the end, procedural generation is presented less as “magic randomness” and more as a design tool: a way to encode decisions and aesthetics into algorithms that act like co-designers alongside humans.

\textbf{Discussion Question.}
If procedural generation is just “rules encoded as programs,” in what ways is designing a procedural generator similar to designing a programming language?

\noindent\textbf{Link:} \url{https://www.youtube.com/watch?v=G6ZHUOSXZDo}


\section{Conclusion}\label{conclusion}

Stepping back from the technical details, this course has quietly reshaped how I think about software engineering as a whole. Most of my other classes focus on building systems in existing languages and tools; CPSC 354 zoomed out and asked a more fundamental question: what \emph{is} a programming language, really, and how do we know what our programs mean? From the MIU puzzle and abstract reduction systems, to lambda calculus, Church numerals, and the \texttt{fix} operator, the course consistently pushed me to see programs as mathematical objects that can be reasoned about in a precise way.

In the wider world of software engineering, this shows up in several important ways. Termination proofs and measure functions are the theoretical side of something every engineer cares about: will this algorithm always finish, and why? Abstract rewriting systems look like toy examples, but they are the same ideas behind compiler optimizations and program transformations: do different rewrite paths produce the same result, or can they get “stuck” in different places? Building and testing the lambda-calculus interpreter connected directly to language implementation and runtime systems. Writing grammars, designing precedence rules, and debugging ambiguous parses is exactly what real language designers and tool builders deal with when they design new syntaxes or write parsers and linters.

What I found most interesting and useful was the combination of \emph{formal} reasoning and \emph{concrete} artifacts. On the one hand, we used Lean to prove small arithmetic and logical facts step by step, seeing how much of “obvious” math actually rests on tiny lemmas like \texttt{add\_zero} and \texttt{add\_succ}. On the other hand, we wrote actual code: an interpreter, tests with \texttt{testing4b.py}, grammar files, and example programs like factorial, Fibonacci, and list-processing with \texttt{hd}/\texttt{tl}. That back-and-forth between proofs and running programs made the material feel less like pure theory and more like a toolkit I can bring into future work: thinking carefully about substitution and variable capture, structuring semantics, and designing tests that reflect the underlying math.

In terms of improvements, I do not have major complaints; the course worked well for me overall. If anything, a small enhancement would be to include a few more explicit “bridges” to mainstream languages and tools (for example, side-by-side comparisons with features in Python, Java, etc), just to help connect the abstract ideas even more directly to the languages most of us use day to day.

\begin{thebibliography}{99}

\bibitem{mukund-lambda}
M.~Mukund,
\emph{The (untyped) lambda calculus},
lecture notes, Chennai Mathematical Institute.
Available at \url{https://www.cmi.ac.in/~madhavan/courses/pl2009/lecturenotes/lecture-notes/node81.html}
(accessed December 4, 2025).

\bibitem{ray-opsem}
J.~Ray,
\emph{Operational Semantics},
Loyola Marymount University.
Available at \url{https://cs.lmu.edu/~ray/notes/opsem/}
(accessed December 4, 2025).

\end{thebibliography}

\end{document}